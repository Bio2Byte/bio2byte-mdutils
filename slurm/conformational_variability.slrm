#!/usr/bin/env bash
#SBATCH --ntasks=8
#SBATCH --mem-per-cpu=8G
#SBATCH --partition=##YOUR_PARTITION##
#SBATCH --time=4:00:00
#SBATCH -J analyze_hbonds

echo ------------------------------------------------------
echo $SLURM_JOB_ID :: $SLURM_JOB_NAME
echo ------------------------------------------------------
echo SLURM: Job submitted from $SLURM_SUBMIT_HOST
echo SLURM: Job is running on $SLURM_JOB_PARTITION
echo -n "SLURM: Job running on node(s): "; echo "$SLURM_NODELIST"
echo SLURM: Working directory is "$SLURM_SUBMIT_DIR"
echo SLURM: Execution mode is $ENVIRONMENT
echo SLURM: \$PATH set to $PATH
echo ------------------------------------------------------

# MAKE SURE ALL MODULES ARE LOADED
# * parallel
# * Python environment with bio2byte-mdutils

cd "$SLURM_SUBMIT_DIR"

# Generate a grid model if needed
PICKLED_MODEL="constava_grid_model.pkl"
if [ ! -f "$PICKLED_MODEL" ]; then
    constava fit-model \ 
        -o "$PICKLED_MODEL" \
        --model-type grid \
        --kde-bandwidth 0.13 \
        --grid-points 10000
fi

# Identify input files
EXEC_SCRIPT="mdutils-constava-wrapper"
INPUT_FILES=( gg*gg/analy/dihedrals.csv )

# Run constava in parallel
#   A special wrapper is used to convert the dihedrals.csv files to the
#   Constava format and run constava with the in-situ generated input
parallel -j $SLURM_NTASKS srun -N 1 -n 1 -c 1 --exact \
    "$EXEC_SCRIPT" \
        -i {} --input-format=csv \
        -o {//}/constava.csv --output-format=csv \
        --window 1 3 5 10 25 50 \
        --bootstrap  3 5 10 25 50 \
        --bootstrap-samples 10000 \
        --load-model="$PICKLED_MODEL" ::: ${INPUT_FILES[@]}
